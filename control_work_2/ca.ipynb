{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unique-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import json\n",
    "import pymorphy2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "included-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data = pd.read_csv(\"Corona_NLP_train.csv\", encoding = \"ISO-8859-1\")\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nasty-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_tweets = list(twitter_data['OriginalTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "korean-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
      "['and and']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_all_tweets[0])\n",
    "\n",
    "print(clean([list_of_all_tweets[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-discrimination",
   "metadata": {},
   "source": [
    "Для начала немного преподготовим наши данные, и объеденим Extremely Negative с Negative  \n",
    "и Extremely Positive с Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "interesting-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral  \n",
       "1  advice Talk to your neighbours family to excha...  Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...  Positive  \n",
       "3  My food stock is not the only one which is emp...  Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Negative  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.loc[twitter_data.Sentiment == 'Extremely Negative', 'Sentiment'] = 'Negative'\n",
    "twitter_data.loc[twitter_data.Sentiment == 'Extremely Positive', 'Sentiment'] = 'Positive'\n",
    "\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-locking",
   "metadata": {},
   "source": [
    "Теперь очистим данные от хэштегов, ссылок и прочего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "compact-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    res = []\n",
    "    for i in range(0, len(data)):\n",
    "        temp = data[i]\n",
    "        temp = re.sub(\"@\\S+\", \" \", temp)\n",
    "        temp = re.sub(\"https*\\S+\", \" \", temp)\n",
    "        temp = re.sub(\"#\\S+\", \" \", temp)\n",
    "        temp = re.sub(\"\\'\\w+\", '', temp)\n",
    "        temp = re.sub('[%s]' % re.escape(string.punctuation), ' ', temp)\n",
    "        temp = re.sub(r'\\w*\\d+\\w*', '', temp)\n",
    "        temp = re.sub('\\s{2,}', \" \", temp)\n",
    "        list_of_temp = temp.split(\" \")\n",
    "        out = []\n",
    "        for temp in list_of_temp:\n",
    "            if not temp.isalpha():\n",
    "                continue\n",
    "            if temp == '':\n",
    "                continue\n",
    "            else:\n",
    "                out.append(temp.lower())\n",
    "        temp = ' '.join(out)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "optimum-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clean(list(twitter_data['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-things",
   "metadata": {},
   "source": [
    "Создадим новый датасет, с которым и будем работать в будущем.  \n",
    "Для определения тональности текста нам не понадобятся username, ScreenName, Location, TweetAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "postal-sally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and and</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me ready to go at supermarket during the outbr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment\n",
       "0                                            and and   Neutral\n",
       "1  advice talk to your neighbours family to excha...  Positive\n",
       "2  coronavirus australia woolworths to give elder...  Positive\n",
       "3  my food stock is not the only one which is emp...  Positive\n",
       "4  me ready to go at supermarket during the outbr...  Negative"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_twitts = pd.DataFrame()\n",
    "\n",
    "clean_twitts['OriginalTweet'] = out\n",
    "clean_twitts['Sentiment'] = twitter_data['Sentiment']\n",
    "\n",
    "clean_twitts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optical-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neutral', 'Positive', 'Negative']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим какие классы у нас вообще есть\n",
    "list_of_labels = list(clean_twitts['Sentiment'].unique())\n",
    "list_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "still-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция разделяет твитты по классам и записывает их в файл, нужно мне для анализа в amazon comprehend\n",
    "def get_twitts(twitter_data, label, filename):\n",
    "    l = twitter_data[twitter_data.Sentiment == label]\n",
    "    l = list(l['OriginalTweet'])\n",
    "    \n",
    "    f = open(f\"{filename}.txt\", \"w\")\n",
    "    for twitt in l[0:int(len(l)/10)]:\n",
    "        f.write(twitt + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saved-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим файл для каждого класса\n",
    "# в результате у нас есть три файла\n",
    "# в каждом файле-классе только твитты относящиеся к этому классу\n",
    "for label in list_of_labels:\n",
    "    get_twitts(clean_twitts, label, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-bouquet",
   "metadata": {},
   "source": [
    "Далее а Amazon Comprehend я для каждого файла сделала анализ на Sentiment, по строчно.  \n",
    "Так как у меня в каждом файле хранятся твитты (разделенные новой строкой), относящиеся только к классу с названием файла, метрику accuracy можно посчитать следующим образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "applied-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(filename, clean=False):\n",
    "    jsons = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            jsons.append(json.loads(line))\n",
    "    \n",
    "    score = 0\n",
    "    for j in jsons:\n",
    "        # сравниваю название файла с предсказаной тональностью (должны совпадать)\n",
    "        original_sentiment = j['File'].split(\".\")[0].lower()\n",
    "        if clean:\n",
    "            original_sentiment = original_sentiment.split(\"_\")[0]\n",
    "        pred_sentiment = j['Sentiment'].lower()\n",
    "        if original_sentiment == pred_sentiment:\n",
    "            score += 1\n",
    "                        \n",
    "    return score / len(jsons) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-color",
   "metadata": {},
   "source": [
    "Как мы видим с помощью амазон сервиса мы получили крайне низкий результат accuracy  \n",
    "Лишь немного выше чем рандомный выбор с вероятностью угадать 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "negative-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 39.499270782693245%\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(\"sentiment_from_comprehend.txt\")\n",
    "print(f\"Accuracy = {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-making",
   "metadata": {},
   "source": [
    "Теперь попробуем всё тоже самое, только удалим стоп-слова и преобразуем все слова в начальную форму.  \n",
    "Для этого создадим ещё один датафрейм, потому что тот нам еще понадобится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "broken-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем в начальную форму и удаляем стоп-слова\n",
    "\n",
    "list_of_twitts = list(clean_twitts['OriginalTweet'])\n",
    "out_list = []\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "for twitt in list_of_twitts:\n",
    "    # удаляем стоп-слова\n",
    "    filtered_words = [word for word in twitt.split(\" \") if word not in stopwords.words('english')]\n",
    "    # преобразуем в начальную форму\n",
    "    normal_form_words = []\n",
    "    for word in filtered_words:\n",
    "        normal_form_words.append(morph.parse(word)[0].normal_form)\n",
    "    # объединяем\n",
    "    out_list.append(' '.join(normal_form_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "excessive-purchase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk neighbours family exchange phone n...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus australia woolworths give elderly ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food stock one empty please panic enough food ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ready go supermarket outbreak paranoid food st...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment\n",
       "0                                                      Neutral\n",
       "1  advice talk neighbours family exchange phone n...  Positive\n",
       "2  coronavirus australia woolworths give elderly ...  Positive\n",
       "3  food stock one empty please panic enough food ...  Positive\n",
       "4  ready go supermarket outbreak paranoid food st...  Negative"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_and_normal = pd.DataFrame()\n",
    "\n",
    "clean_and_normal['OriginalTweet'] = out_list\n",
    "clean_and_normal['Sentiment'] = twitter_data['Sentiment']\n",
    "\n",
    "clean_and_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "orange-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим такие же файлы как и раньше\n",
    "for label in list_of_labels:\n",
    "    get_twitts(clean_and_normal, label, label + '_cleanup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-campus",
   "metadata": {},
   "source": [
    "Далее всё тоже самое с Amazon Comprehend  \n",
    "Как можно наблюдать точность уменьшилась, что меня несколько удивило"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cooked-bride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 31.186193485658727%\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(\"output\", True)\n",
    "print(f\"Accuracy = {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-seven",
   "metadata": {},
   "source": [
    "Теперь попробуем написать что-то своё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acquired-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# нам нужна функция которая посчитает метрику accuracy схожим способом что и раньше\n",
    "def get_accuracy_from_list(real, predictions):\n",
    "    score = 0\n",
    "    for i in range(len(real)):\n",
    "        if real[i] == predictions[i]:\n",
    "            score += 1\n",
    "    return score / len(real) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-steering",
   "metadata": {},
   "source": [
    "Далее поделим наши данные на тестовую и обучающие выборки, сначала для данных, очищенных только от хэштегов, ссылок и прочего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "antique-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_twitts['OriginalTweet'], clean_twitts['Sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-frame",
   "metadata": {},
   "source": [
    "Преобразуем данные в вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adequate-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "immune-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(list(X_train)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "brilliant-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(LogisticRegression(), train_data_features, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-sucking",
   "metadata": {},
   "source": [
    "Как можно наблюдать даже простая логистическая регрессия справилась с нашей задачей куда лучаше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "lonely-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-ticket",
   "metadata": {},
   "source": [
    "Всё тоже самое, только для нормализованных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "altered-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_and_normal['OriginalTweet'], clean_and_normal['Sentiment'], test_size=0.2, random_state=42)\n",
    "train_data_features = vectorizer.fit_transform(list(X_train)).toarray()\n",
    "scores = cross_val_score(LogisticRegression(), train_data_features, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "subject-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-explanation",
   "metadata": {},
   "source": [
    "С RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "statistical-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "train_data_features = vectorizer.fit_transform(list(X_train)).toarray()\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators = 200), train_data_features, y_train, cv=5)\n",
    "print(\"Accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "according-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 5000    # Word vector dimensionality                      \n",
    "min_word_count = 3    # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = Word2Vec(list(clean_and_normal['OriginalTweet']), workers=num_workers, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "model_name = \"clean_and_normail_data\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "official-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "thirty-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f1ba1c9c26bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_and_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-ac7b3b26ed98>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[0;34m(reviews, model, num_features)\u001b[0m\n\u001b[1;32m     44\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m        \u001b[0;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n\u001b[0m\u001b[1;32m     47\u001b[0m            num_features)\n\u001b[1;32m     48\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-ac7b3b26ed98>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[0;34m(words, model, num_features)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex2word_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mnwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mfeatureVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureVec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Divide the result by the number of words to get the average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( list(clean_and_normal), model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "widespread-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = dict()\n",
    "\n",
    "for tweet in list(clean_twitts['OriginalTweet']):\n",
    "    words = tweet.split(' ')\n",
    "    for word in words:\n",
    "        if most_frequent.get(word) is None:\n",
    "            most_frequent[word] = 1\n",
    "        else:\n",
    "            most_frequent[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "guided-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32184"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "hollow-monaco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 44771),\n",
       " ('to', 38457),\n",
       " ('and', 24060),\n",
       " ('of', 21543),\n",
       " ('a', 19426),\n",
       " ('in', 19295),\n",
       " ('for', 14047),\n",
       " ('is', 12256),\n",
       " ('are', 11339),\n",
       " ('covid', 10545),\n",
       " ('i', 10026),\n",
       " ('you', 9705),\n",
       " ('on', 9432),\n",
       " ('this', 7970),\n",
       " ('at', 7842),\n",
       " ('prices', 7841),\n",
       " ('store', 6878),\n",
       " ('food', 6859),\n",
       " ('supermarket', 6692),\n",
       " ('we', 6648),\n",
       " ('with', 6631),\n",
       " ('it', 6612),\n",
       " ('that', 6494),\n",
       " ('have', 6157),\n",
       " ('as', 6101),\n",
       " ('grocery', 6094),\n",
       " ('be', 5745),\n",
       " ('people', 5547),\n",
       " ('from', 5299),\n",
       " ('amp', 5187),\n",
       " ('all', 4808),\n",
       " ('your', 4597),\n",
       " ('not', 4537),\n",
       " ('will', 4495),\n",
       " ('consumer', 4402),\n",
       " ('my', 4201),\n",
       " ('can', 4194),\n",
       " ('they', 4098),\n",
       " ('our', 4043),\n",
       " ('up', 3876),\n",
       " ('out', 3846),\n",
       " ('has', 3800),\n",
       " ('or', 3732),\n",
       " ('by', 3720),\n",
       " ('more', 3712),\n",
       " ('but', 3564),\n",
       " ('if', 3536),\n",
       " ('shopping', 3367),\n",
       " ('online', 3357),\n",
       " ('how', 3322),\n",
       " ('their', 3223),\n",
       " ('during', 3203),\n",
       " ('so', 3106),\n",
       " ('now', 3101),\n",
       " ('no', 2988),\n",
       " ('get', 2871),\n",
       " ('about', 2833),\n",
       " ('what', 2818),\n",
       " ('need', 2712),\n",
       " ('who', 2705),\n",
       " ('pandemic', 2699),\n",
       " ('just', 2588),\n",
       " ('workers', 2586),\n",
       " ('panic', 2536),\n",
       " ('do', 2402),\n",
       " ('like', 2370),\n",
       " ('time', 2363),\n",
       " ('us', 2324),\n",
       " ('was', 2310),\n",
       " ('an', 2292),\n",
       " ('there', 2291),\n",
       " ('home', 2288),\n",
       " ('when', 2285),\n",
       " ('go', 2279),\n",
       " ('demand', 2271),\n",
       " ('s', 2187),\n",
       " ('coronavirus', 2163),\n",
       " ('some', 2163),\n",
       " ('sanitizer', 2161),\n",
       " ('help', 2105),\n",
       " ('hand', 2095),\n",
       " ('stock', 1980),\n",
       " ('going', 1951),\n",
       " ('one', 1941),\n",
       " ('me', 1915),\n",
       " ('due', 1834),\n",
       " ('here', 1833),\n",
       " ('been', 1785),\n",
       " ('buying', 1769),\n",
       " ('new', 1717),\n",
       " ('please', 1672),\n",
       " ('work', 1623),\n",
       " ('crisis', 1619),\n",
       " ('them', 1591),\n",
       " ('after', 1532),\n",
       " ('other', 1530),\n",
       " ('oil', 1530),\n",
       " ('only', 1514),\n",
       " ('because', 1510),\n",
       " ('toilet', 1505),\n",
       " ('these', 1492),\n",
       " ('should', 1475),\n",
       " ('than', 1474),\n",
       " ('paper', 1463),\n",
       " ('over', 1450),\n",
       " ('stay', 1436),\n",
       " ('local', 1428),\n",
       " ('today', 1417),\n",
       " ('buy', 1365),\n",
       " ('keep', 1364),\n",
       " ('via', 1355),\n",
       " ('many', 1344),\n",
       " ('stores', 1337),\n",
       " ('shelves', 1331),\n",
       " ('still', 1320),\n",
       " ('being', 1319),\n",
       " ('don', 1301),\n",
       " ('make', 1300),\n",
       " ('see', 1272),\n",
       " ('down', 1254),\n",
       " ('those', 1251),\n",
       " ('delivery', 1251),\n",
       " ('know', 1204),\n",
       " ('day', 1190),\n",
       " ('why', 1175),\n",
       " ('take', 1169),\n",
       " ('supply', 1143),\n",
       " ('its', 1140),\n",
       " ('would', 1138),\n",
       " ('retail', 1124),\n",
       " ('stop', 1122),\n",
       " ('staff', 1120),\n",
       " ('into', 1120),\n",
       " ('outbreak', 1114),\n",
       " ('any', 1090),\n",
       " ('even', 1080),\n",
       " ('everyone', 1076),\n",
       " ('social', 1076),\n",
       " ('had', 1076),\n",
       " ('t', 1075),\n",
       " ('could', 1072),\n",
       " ('price', 1057),\n",
       " ('while', 1054),\n",
       " ('also', 1047),\n",
       " ('working', 1033),\n",
       " ('week', 1002),\n",
       " ('virus', 998),\n",
       " ('essential', 990),\n",
       " ('health', 988),\n",
       " ('masks', 987),\n",
       " ('think', 964),\n",
       " ('world', 955),\n",
       " ('amid', 954),\n",
       " ('good', 954),\n",
       " ('spread', 953),\n",
       " ('use', 951),\n",
       " ('market', 950),\n",
       " ('safe', 931),\n",
       " ('were', 931),\n",
       " ('most', 923),\n",
       " ('shop', 920),\n",
       " ('customers', 912),\n",
       " ('way', 911),\n",
       " ('thank', 905),\n",
       " ('he', 901),\n",
       " ('business', 899),\n",
       " ('off', 884),\n",
       " ('which', 878),\n",
       " ('employees', 875),\n",
       " ('right', 874),\n",
       " ('products', 871),\n",
       " ('impact', 869),\n",
       " ('back', 866),\n",
       " ('much', 861),\n",
       " ('may', 859),\n",
       " ('government', 856),\n",
       " ('every', 856),\n",
       " ('first', 854),\n",
       " ('before', 852),\n",
       " ('doing', 847),\n",
       " ('items', 845),\n",
       " ('times', 825),\n",
       " ('supplies', 816),\n",
       " ('last', 800),\n",
       " ('lockdown', 780),\n",
       " ('where', 774),\n",
       " ('read', 771),\n",
       " ('free', 769),\n",
       " ('support', 758),\n",
       " ('well', 753),\n",
       " ('high', 748),\n",
       " ('face', 747),\n",
       " ('find', 740),\n",
       " ('low', 740),\n",
       " ('want', 737),\n",
       " ('days', 736),\n",
       " ('around', 734),\n",
       " ('then', 728),\n",
       " ('mask', 726),\n",
       " ('through', 725),\n",
       " ('got', 723),\n",
       " ('said', 719),\n",
       " ('global', 718),\n",
       " ('weeks', 707),\n",
       " ('news', 706),\n",
       " ('distancing', 701),\n",
       " ('public', 695),\n",
       " ('risk', 687),\n",
       " ('too', 684),\n",
       " ('empty', 683),\n",
       " ('open', 683),\n",
       " ('u', 681),\n",
       " ('says', 680),\n",
       " ('getting', 671),\n",
       " ('gas', 669),\n",
       " ('really', 667),\n",
       " ('companies', 665),\n",
       " ('am', 664),\n",
       " ('since', 660),\n",
       " ('went', 659),\n",
       " ('very', 657),\n",
       " ('businesses', 653),\n",
       " ('long', 652),\n",
       " ('country', 647),\n",
       " ('order', 642),\n",
       " ('care', 640),\n",
       " ('say', 635),\n",
       " ('money', 632),\n",
       " ('making', 631),\n",
       " ('two', 628),\n",
       " ('next', 627),\n",
       " ('things', 624),\n",
       " ('great', 624),\n",
       " ('she', 622),\n",
       " ('her', 618),\n",
       " ('enough', 615),\n",
       " ('line', 615),\n",
       " ('supermarkets', 612),\n",
       " ('his', 611),\n",
       " ('consumers', 611),\n",
       " ('look', 607),\n",
       " ('full', 597),\n",
       " ('others', 597),\n",
       " ('available', 595),\n",
       " ('let', 592),\n",
       " ('re', 587),\n",
       " ('march', 585),\n",
       " ('pay', 584),\n",
       " ('check', 581),\n",
       " ('service', 580),\n",
       " ('services', 579),\n",
       " ('shops', 575),\n",
       " ('hands', 575),\n",
       " ('taking', 571),\n",
       " ('hours', 565),\n",
       " ('uk', 563),\n",
       " ('family', 559),\n",
       " ('economy', 558),\n",
       " ('against', 555),\n",
       " ('anyone', 554),\n",
       " ('gloves', 551),\n",
       " ('chain', 548),\n",
       " ('come', 547),\n",
       " ('put', 544),\n",
       " ('year', 544),\n",
       " ('across', 543),\n",
       " ('response', 542),\n",
       " ('must', 533),\n",
       " ('industry', 531),\n",
       " ('medical', 530),\n",
       " ('house', 520),\n",
       " ('increase', 518),\n",
       " ('needs', 517),\n",
       " ('few', 516),\n",
       " ('life', 509),\n",
       " ('everything', 503),\n",
       " ('goods', 502),\n",
       " ('state', 497),\n",
       " ('protect', 495),\n",
       " ('report', 495),\n",
       " ('without', 494),\n",
       " ('trying', 494),\n",
       " ('fight', 493),\n",
       " ('self', 493),\n",
       " ('closed', 493),\n",
       " ('big', 491),\n",
       " ('same', 490),\n",
       " ('etc', 487),\n",
       " ('best', 485),\n",
       " ('increased', 481),\n",
       " ('never', 478),\n",
       " ('already', 478),\n",
       " ('sure', 473),\n",
       " ('thing', 471),\n",
       " ('markets', 471),\n",
       " ('emergency', 470),\n",
       " ('per', 468),\n",
       " ('close', 467),\n",
       " ('made', 464),\n",
       " ('post', 464),\n",
       " ('give', 462),\n",
       " ('thanks', 462),\n",
       " ('economic', 462),\n",
       " ('drivers', 461),\n",
       " ('small', 457),\n",
       " ('vulnerable', 457),\n",
       " ('call', 456),\n",
       " ('each', 451),\n",
       " ('did', 451),\n",
       " ('place', 448),\n",
       " ('live', 447),\n",
       " ('elderly', 446),\n",
       " ('someone', 444),\n",
       " ('community', 443),\n",
       " ('real', 443),\n",
       " ('seen', 442),\n",
       " ('having', 442),\n",
       " ('sales', 440),\n",
       " ('better', 440),\n",
       " ('such', 438),\n",
       " ('groceries', 436),\n",
       " ('continue', 435),\n",
       " ('situation', 435),\n",
       " ('latest', 431),\n",
       " ('does', 427),\n",
       " ('using', 426),\n",
       " ('behavior', 425),\n",
       " ('lot', 425),\n",
       " ('hoarding', 425),\n",
       " ('another', 424),\n",
       " ('hard', 422),\n",
       " ('job', 422),\n",
       " ('looking', 422),\n",
       " ('person', 417),\n",
       " ('shoppers', 416),\n",
       " ('trump', 415),\n",
       " ('cases', 415),\n",
       " ('banks', 414),\n",
       " ('away', 414),\n",
       " ('quarantine', 413),\n",
       " ('share', 412),\n",
       " ('scams', 412),\n",
       " ('hope', 410),\n",
       " ('part', 407),\n",
       " ('bank', 405),\n",
       " ('important', 405),\n",
       " ('coming', 404),\n",
       " ('under', 401),\n",
       " ('avoid', 401),\n",
       " ('hit', 401),\n",
       " ('morning', 400),\n",
       " ('gt', 400),\n",
       " ('able', 400),\n",
       " ('together', 398),\n",
       " ('left', 397),\n",
       " ('own', 397),\n",
       " ('selling', 397),\n",
       " ('front', 393),\n",
       " ('production', 392),\n",
       " ('daily', 391),\n",
       " ('drop', 391),\n",
       " ('learn', 391),\n",
       " ('china', 389),\n",
       " ('information', 389),\n",
       " ('data', 389),\n",
       " ('water', 388),\n",
       " ('ever', 377),\n",
       " ('protection', 375),\n",
       " ('end', 375),\n",
       " ('run', 374),\n",
       " ('retailers', 374),\n",
       " ('change', 374),\n",
       " ('current', 372),\n",
       " ('list', 371),\n",
       " ('until', 369),\n",
       " ('might', 367),\n",
       " ('spending', 367),\n",
       " ('measures', 365),\n",
       " ('normal', 364),\n",
       " ('company', 364),\n",
       " ('else', 364),\n",
       " ('less', 363),\n",
       " ('healthcare', 363),\n",
       " ('positive', 361),\n",
       " ('key', 359),\n",
       " ('start', 359),\n",
       " ('outside', 358),\n",
       " ('financial', 358),\n",
       " ('least', 358),\n",
       " ('again', 358),\n",
       " ('essentials', 357),\n",
       " ('feel', 354),\n",
       " ('leave', 352),\n",
       " ('done', 347),\n",
       " ('wash', 347),\n",
       " ('told', 347),\n",
       " ('little', 345),\n",
       " ('seeing', 344),\n",
       " ('man', 342),\n",
       " ('nothing', 342),\n",
       " ('thought', 341),\n",
       " ('safety', 340),\n",
       " ('remember', 340),\n",
       " ('milk', 338),\n",
       " ('related', 337),\n",
       " ('years', 336),\n",
       " ('corona', 333),\n",
       " ('months', 333),\n",
       " ('cut', 332),\n",
       " ('wearing', 332),\n",
       " ('tips', 331),\n",
       " ('old', 331),\n",
       " ('contact', 330),\n",
       " ('jobs', 330),\n",
       " ('something', 330),\n",
       " ('meet', 329),\n",
       " ('police', 326),\n",
       " ('yet', 326),\n",
       " ('w', 326),\n",
       " ('month', 325),\n",
       " ('lives', 325),\n",
       " ('fall', 325),\n",
       " ('families', 319),\n",
       " ('keeping', 318),\n",
       " ('helping', 317),\n",
       " ('including', 316),\n",
       " ('april', 316),\n",
       " ('video', 315),\n",
       " ('shortage', 314),\n",
       " ('sick', 313),\n",
       " ('access', 313),\n",
       " ('m', 312),\n",
       " ('worker', 311),\n",
       " ('roll', 309),\n",
       " ('running', 309),\n",
       " ('chains', 308),\n",
       " ('wear', 308),\n",
       " ('reports', 306),\n",
       " ('possible', 306),\n",
       " ('rise', 306),\n",
       " ('lower', 305),\n",
       " ('soap', 303),\n",
       " ('update', 300),\n",
       " ('used', 299),\n",
       " ('try', 297),\n",
       " ('higher', 296),\n",
       " ('needed', 296),\n",
       " ('always', 296),\n",
       " ('bad', 296),\n",
       " ('distance', 296),\n",
       " ('alcohol', 296),\n",
       " ('restaurants', 295),\n",
       " ('visit', 294),\n",
       " ('nhs', 294),\n",
       " ('doctors', 293),\n",
       " ('lines', 292),\n",
       " ('yourself', 292),\n",
       " ('instead', 292),\n",
       " ('americans', 292),\n",
       " ('farmers', 292),\n",
       " ('amazon', 291),\n",
       " ('provide', 291),\n",
       " ('based', 291),\n",
       " ('likely', 291),\n",
       " ('set', 290),\n",
       " ('found', 290),\n",
       " ('number', 289),\n",
       " ('soon', 288),\n",
       " ('city', 288),\n",
       " ('e', 287),\n",
       " ('prevent', 286),\n",
       " ('bought', 285),\n",
       " ('changes', 285),\n",
       " ('stocked', 284),\n",
       " ('non', 284),\n",
       " ('major', 283),\n",
       " ('offering', 281),\n",
       " ('once', 280),\n",
       " ('shortages', 280),\n",
       " ('drive', 279),\n",
       " ('media', 279),\n",
       " ('watch', 279),\n",
       " ('putting', 278),\n",
       " ('anything', 277),\n",
       " ('advice', 276),\n",
       " ('hospital', 276),\n",
       " ('travel', 274),\n",
       " ('relief', 273),\n",
       " ('govt', 273),\n",
       " ('testing', 272),\n",
       " ('between', 272),\n",
       " ('nurses', 272),\n",
       " ('far', 271),\n",
       " ('pharmacy', 271),\n",
       " ('produce', 271),\n",
       " ('orders', 270),\n",
       " ('yesterday', 269),\n",
       " ('friends', 269),\n",
       " ('especially', 268),\n",
       " ('hour', 267),\n",
       " ('india', 267),\n",
       " ('advantage', 266),\n",
       " ('almost', 261),\n",
       " ('gouging', 261),\n",
       " ('states', 260),\n",
       " ('gone', 260),\n",
       " ('affected', 260),\n",
       " ('ensure', 259),\n",
       " ('special', 259),\n",
       " ('deal', 258),\n",
       " ('website', 258),\n",
       " ('paid', 257),\n",
       " ('join', 257),\n",
       " ('car', 256),\n",
       " ('sanitizers', 255),\n",
       " ('whole', 253),\n",
       " ('saw', 253),\n",
       " ('result', 253),\n",
       " ('understand', 252),\n",
       " ('isolation', 251),\n",
       " ('kids', 251),\n",
       " ('following', 251),\n",
       " ('system', 251),\n",
       " ('sell', 251),\n",
       " ('follow', 251),\n",
       " ('million', 250),\n",
       " ('top', 249),\n",
       " ('staying', 249),\n",
       " ('love', 249),\n",
       " ('reduce', 249),\n",
       " ('selfish', 248),\n",
       " ('healthy', 247),\n",
       " ('won', 247),\n",
       " ('info', 247),\n",
       " ('control', 247),\n",
       " ('area', 246),\n",
       " ('means', 246),\n",
       " ('fear', 246),\n",
       " ('show', 246),\n",
       " ('eat', 246),\n",
       " ('tested', 245),\n",
       " ('test', 245),\n",
       " ('fresh', 244),\n",
       " ('actually', 244),\n",
       " ('customer', 243),\n",
       " ('remain', 243),\n",
       " ('surge', 243),\n",
       " ('wait', 242),\n",
       " ('brands', 242),\n",
       " ('story', 242),\n",
       " ('case', 241),\n",
       " ('meat', 241),\n",
       " ('article', 241),\n",
       " ('below', 239),\n",
       " ('shut', 239),\n",
       " ('die', 238),\n",
       " ('caused', 238),\n",
       " ('tell', 238),\n",
       " ('save', 238),\n",
       " ('credit', 237),\n",
       " ('countries', 237),\n",
       " ('despite', 237),\n",
       " ('sold', 236),\n",
       " ('war', 236),\n",
       " ('stuff', 235),\n",
       " ('rolls', 235),\n",
       " ('point', 234),\n",
       " ('works', 234),\n",
       " ('announced', 234),\n",
       " ('huge', 234),\n",
       " ('cannot', 233),\n",
       " ('ask', 232),\n",
       " ('cost', 231),\n",
       " ('extra', 231),\n",
       " ('three', 231),\n",
       " ('kind', 230),\n",
       " ('bread', 230),\n",
       " ('given', 230),\n",
       " ('saying', 229),\n",
       " ('maybe', 229),\n",
       " ('woman', 229),\n",
       " ('period', 229),\n",
       " ('early', 228),\n",
       " ('waiting', 228),\n",
       " ('cash', 227),\n",
       " ('limit', 227),\n",
       " ('themselves', 226),\n",
       " ('critical', 225),\n",
       " ('trip', 225),\n",
       " ('moment', 225),\n",
       " ('asked', 224),\n",
       " ('facing', 222),\n",
       " ('difficult', 222),\n",
       " ('yes', 221),\n",
       " ('changing', 221),\n",
       " ('product', 221),\n",
       " ('idea', 220),\n",
       " ('ve', 220),\n",
       " ('ago', 219),\n",
       " ('personal', 219),\n",
       " ('increasing', 218),\n",
       " ('short', 218),\n",
       " ('different', 218),\n",
       " ('struggling', 217),\n",
       " ('him', 217),\n",
       " ('taken', 217),\n",
       " ('become', 217),\n",
       " ('drug', 217),\n",
       " ('employee', 216),\n",
       " ('ways', 216),\n",
       " ('heroes', 216),\n",
       " ('basic', 215),\n",
       " ('both', 215),\n",
       " ('pick', 215),\n",
       " ('spreading', 215),\n",
       " ('currently', 215),\n",
       " ('shit', 215),\n",
       " ('bring', 215),\n",
       " ('queue', 215),\n",
       " ('tp', 214),\n",
       " ('mean', 213),\n",
       " ('security', 213),\n",
       " ('national', 213),\n",
       " ('sector', 211),\n",
       " ('lost', 210),\n",
       " ('seriously', 210),\n",
       " ('purchase', 209),\n",
       " ('cleaning', 209),\n",
       " ('shift', 209),\n",
       " ('fast', 209),\n",
       " ('consider', 208),\n",
       " ('clean', 208),\n",
       " ('term', 208),\n",
       " ('n', 207),\n",
       " ('survive', 207),\n",
       " ('started', 207),\n",
       " ('experts', 207),\n",
       " ('inside', 207),\n",
       " ('sign', 206),\n",
       " ('cause', 205),\n",
       " ('further', 205),\n",
       " ('changed', 205),\n",
       " ('closing', 204),\n",
       " ('crazy', 204),\n",
       " ('buyers', 204),\n",
       " ('deliveries', 203),\n",
       " ('restrictions', 203),\n",
       " ('shows', 203),\n",
       " ('president', 203),\n",
       " ('giving', 203),\n",
       " ('restaurant', 202),\n",
       " ('america', 202),\n",
       " ('tomorrow', 202),\n",
       " ('reduced', 202),\n",
       " ('fuel', 202),\n",
       " ('offer', 201),\n",
       " ('eggs', 201),\n",
       " ('large', 201),\n",
       " ('ppl', 201),\n",
       " ('plan', 200),\n",
       " ('act', 200),\n",
       " ('behaviour', 200),\n",
       " ('allowed', 200),\n",
       " ('guys', 200),\n",
       " ('issues', 199),\n",
       " ('fears', 199),\n",
       " ('makes', 198),\n",
       " ('nation', 198),\n",
       " ('team', 197),\n",
       " ('future', 197),\n",
       " ('group', 197),\n",
       " ('recent', 197),\n",
       " ('scammers', 197),\n",
       " ('asking', 195),\n",
       " ('providing', 194),\n",
       " ('research', 194),\n",
       " ('wrong', 193),\n",
       " ('office', 192),\n",
       " ('american', 192),\n",
       " ('starting', 192),\n",
       " ('link', 191),\n",
       " ('school', 191),\n",
       " ('fucking', 191),\n",
       " ('action', 191),\n",
       " ('closures', 190),\n",
       " ('goes', 190),\n",
       " ('plenty', 190),\n",
       " ('hey', 190),\n",
       " ('donate', 189),\n",
       " ('confidence', 188),\n",
       " ('concerns', 188),\n",
       " ('doesn', 188),\n",
       " ('walk', 187),\n",
       " ('near', 187),\n",
       " ('folks', 187),\n",
       " ('problem', 187),\n",
       " ('continues', 187),\n",
       " ('worried', 186),\n",
       " ('gonna', 186),\n",
       " ('thinking', 186),\n",
       " ('trade', 186),\n",
       " ('italy', 186),\n",
       " ('ones', 184),\n",
       " ('worse', 184),\n",
       " ('strong', 184),\n",
       " ('falling', 184),\n",
       " ('bit', 184),\n",
       " ('ready', 183),\n",
       " ('past', 183),\n",
       " ('isn', 183),\n",
       " ('wipes', 183),\n",
       " ('russia', 182),\n",
       " ('energy', 182),\n",
       " ('came', 181),\n",
       " ('deliver', 181),\n",
       " ('trends', 181),\n",
       " ('general', 181),\n",
       " ('raise', 181),\n",
       " ('longer', 180),\n",
       " ('living', 180),\n",
       " ('hear', 180),\n",
       " ('limited', 180),\n",
       " ('questions', 180),\n",
       " ('according', 179),\n",
       " ('half', 179),\n",
       " ('direct', 179),\n",
       " ('afford', 178),\n",
       " ('email', 178),\n",
       " ('believe', 178),\n",
       " ('pasta', 178),\n",
       " ('resources', 178),\n",
       " ('behind', 178),\n",
       " ('though', 177),\n",
       " ('probably', 177),\n",
       " ('fighting', 177),\n",
       " ('door', 177),\n",
       " ('expect', 176),\n",
       " ('household', 176),\n",
       " ('rs', 176),\n",
       " ('expected', 176),\n",
       " ('didn', 175),\n",
       " ('updates', 175),\n",
       " ('died', 175),\n",
       " ('equipment', 175),\n",
       " ('canada', 174),\n",
       " ('single', 173),\n",
       " ('chinese', 173),\n",
       " ('b', 173),\n",
       " ('god', 173),\n",
       " ('allow', 173),\n",
       " ('ceo', 172),\n",
       " ('night', 172),\n",
       " ('send', 172),\n",
       " ('looks', 172),\n",
       " ('fuck', 172),\n",
       " ('delivered', 172),\n",
       " ('federal', 171),\n",
       " ('crude', 171),\n",
       " ('law', 171),\n",
       " ('bottle', 169),\n",
       " ('poor', 169),\n",
       " ('citizens', 169),\n",
       " ('members', 169),\n",
       " ('lots', 169),\n",
       " ('truck', 169),\n",
       " ('necessary', 168),\n",
       " ('side', 168),\n",
       " ('friend', 168),\n",
       " ('foods', 167),\n",
       " ('happy', 167),\n",
       " ('nice', 167),\n",
       " ('rules', 166),\n",
       " ('seems', 166),\n",
       " ('income', 166),\n",
       " ('message', 166),\n",
       " ('called', 165),\n",
       " ('recession', 165),\n",
       " ('efforts', 165),\n",
       " ('pack', 165),\n",
       " ('sale', 165),\n",
       " ('experience', 164),\n",
       " ('light', 164),\n",
       " ('places', 164),\n",
       " ('d', 163),\n",
       " ('plans', 163),\n",
       " ('effects', 163),\n",
       " ('stocks', 163),\n",
       " ('tests', 163),\n",
       " ('forced', 163),\n",
       " ('parents', 162),\n",
       " ('comes', 162),\n",
       " ('question', 162),\n",
       " ('reason', 162),\n",
       " ('county', 161),\n",
       " ('insights', 161),\n",
       " ('monday', 161),\n",
       " ('ok', 161),\n",
       " ('serious', 160),\n",
       " ('symptoms', 160),\n",
       " ('potential', 160),\n",
       " ('calls', 160),\n",
       " ('hoard', 159),\n",
       " ('rice', 159),\n",
       " ('habits', 159),\n",
       " ('issue', 159),\n",
       " ('gets', 159),\n",
       " ('hospitals', 159),\n",
       " ('aren', 158),\n",
       " ('south', 158),\n",
       " ('frontline', 158),\n",
       " ('biggest', 158),\n",
       " ('took', 158),\n",
       " ('slots', 158),\n",
       " ('opening', 158),\n",
       " ('survey', 158),\n",
       " ('unprecedented', 158),\n",
       " ('rising', 158),\n",
       " ('phone', 157),\n",
       " ('click', 157),\n",
       " ('amount', 157),\n",
       " ('alert', 157),\n",
       " ('gov', 157),\n",
       " ('apart', 157),\n",
       " ('however', 156),\n",
       " ('worth', 156),\n",
       " ('marketing', 156),\n",
       " ('finally', 156),\n",
       " ('kill', 156),\n",
       " ('human', 156),\n",
       " ('wake', 155),\n",
       " ('pm', 155),\n",
       " ('society', 155),\n",
       " ('listen', 155),\n",
       " ('gold', 155),\n",
       " ('beginning', 154),\n",
       " ('massive', 154),\n",
       " ('responders', 154),\n",
       " ('inflated', 154),\n",
       " ('move', 154),\n",
       " ('walmart', 154),\n",
       " ('interesting', 154),\n",
       " ('talk', 153),\n",
       " ('usual', 153),\n",
       " ('effect', 153),\n",
       " ('hiking', 153),\n",
       " ('blog', 153),\n",
       " ('saudi', 153),\n",
       " ('level', 152),\n",
       " ('infected', 152),\n",
       " ('death', 152),\n",
       " ('residents', 152),\n",
       " ('c', 152),\n",
       " ('bags', 152),\n",
       " ('seniors', 151),\n",
       " ('street', 151),\n",
       " ('collapse', 151),\n",
       " ('driving', 151),\n",
       " ('rather', 150),\n",
       " ('ppe', 150),\n",
       " ('temporarily', 149),\n",
       " ('clear', 149),\n",
       " ('true', 149),\n",
       " ('disease', 149),\n",
       " ('distribution', 149),\n",
       " ('â', 149),\n",
       " ('amazing', 148),\n",
       " ('forget', 148),\n",
       " ('washing', 148),\n",
       " ('policy', 148),\n",
       " ('growing', 148),\n",
       " ('several', 147),\n",
       " ('y', 147),\n",
       " ('communities', 147),\n",
       " ('homes', 147),\n",
       " ('pretty', 146),\n",
       " ('dont', 146),\n",
       " ('happen', 146),\n",
       " ('cure', 146),\n",
       " ('digital', 146),\n",
       " ('dear', 145),\n",
       " ('details', 145),\n",
       " ('nurse', 145),\n",
       " ('anxiety', 145),\n",
       " ('sentiment', 145),\n",
       " ('touch', 145),\n",
       " ('coughing', 144),\n",
       " ('rate', 144),\n",
       " ('donations', 144),\n",
       " ('record', 144),\n",
       " ('calling', 143),\n",
       " ('bill', 143),\n",
       " ('debt', 143),\n",
       " ('easy', 143),\n",
       " ('stockpiling', 143),\n",
       " ('myself', 143),\n",
       " ('hold', 143),\n",
       " ('among', 143),\n",
       " ('hygiene', 142),\n",
       " ('couple', 142),\n",
       " ('profit', 142),\n",
       " ('spend', 141),\n",
       " ('step', 141),\n",
       " ('feed', 140),\n",
       " ('shelf', 140),\n",
       " ('impacted', 140),\n",
       " ('guess', 140),\n",
       " ('regarding', 140),\n",
       " ('growth', 140),\n",
       " ('children', 140),\n",
       " ('rent', 140),\n",
       " ('showing', 140),\n",
       " ('costs', 140),\n",
       " ('force', 140),\n",
       " ('hi', 139),\n",
       " ('second', 139),\n",
       " ('australia', 138),\n",
       " ('fake', 138),\n",
       " ('minister', 138),\n",
       " ('talking', 137),\n",
       " ('cheap', 137),\n",
       " ('department', 137),\n",
       " ('package', 137),\n",
       " ('dr', 137),\n",
       " ('challenges', 136),\n",
       " ('later', 136),\n",
       " ('r', 136),\n",
       " ('mind', 135),\n",
       " ('stand', 135),\n",
       " ('commerce', 135),\n",
       " ('quick', 135),\n",
       " ('note', 135),\n",
       " ('sir', 135),\n",
       " ('four', 135),\n",
       " ('calm', 134),\n",
       " ('donating', 134),\n",
       " ('head', 134),\n",
       " ('tonight', 134),\n",
       " ('giant', 134),\n",
       " ('oh', 134),\n",
       " ('quickly', 133),\n",
       " ('notice', 133),\n",
       " ('benefits', 133),\n",
       " ('turn', 133),\n",
       " ('protective', 133),\n",
       " ('bulk', 133),\n",
       " ('card', 133),\n",
       " ('dropped', 133),\n",
       " ('clerks', 133),\n",
       " ('simple', 133),\n",
       " ('deaths', 133),\n",
       " ('impacts', 132),\n",
       " ('priority', 132),\n",
       " ('meals', 132),\n",
       " ('worry', 132),\n",
       " ('fruit', 132),\n",
       " ('along', 132),\n",
       " ('paying', 132),\n",
       " ('weekly', 132),\n",
       " ('unemployment', 132),\n",
       " ('certain', 132),\n",
       " ('fact', 131),\n",
       " ('aisle', 131),\n",
       " ('plus', 131),\n",
       " ('imagine', 131),\n",
       " ('takes', 131),\n",
       " ('power', 131),\n",
       " ('thousands', 131),\n",
       " ('date', 130),\n",
       " ('increases', 130),\n",
       " ('worst', 130),\n",
       " ('steps', 130),\n",
       " ('ahead', 130),\n",
       " ('millions', 130),\n",
       " ('app', 129),\n",
       " ('wonder', 129),\n",
       " ('rest', 129),\n",
       " ('groups', 129),\n",
       " ('mass', 129),\n",
       " ('areas', 129),\n",
       " ('shame', 129),\n",
       " ('aisles', 129),\n",
       " ('age', 129),\n",
       " ('eating', 128),\n",
       " ('either', 128),\n",
       " ('sense', 128),\n",
       " ('absolutely', 128),\n",
       " ('vegetables', 128),\n",
       " ('within', 128),\n",
       " ('insurance', 128),\n",
       " ('commodities', 128),\n",
       " ('baby', 128),\n",
       " ('rights', 128),\n",
       " ('producers', 128),\n",
       " ('whether', 127),\n",
       " ('wanted', 127),\n",
       " ('include', 127),\n",
       " ('wants', 127),\n",
       " ('patients', 127),\n",
       " ('created', 127),\n",
       " ('lock', 127),\n",
       " ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(most_frequent.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-concentrate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
